{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBuFJ5RI7H2H"
      },
      "source": [
        "# Image Captioning Version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Cj9olKH7CP0"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-mAAgBo7u0c",
        "outputId": "4b105168-8cf4-436a-e446-c4f899ff48d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 35.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 71.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 78.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-9cmld7qj\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-9cmld7qj\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.12.0+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369389 sha256=74c57d1504df19e7fb60863cbadfc0ebb9b14548f88de4330d0b37380cda6394\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7zfx1u0g/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2vKcr9oSMUQ",
        "outputId": "f8bd673f-aab0-48c6-bf69-d785464dea8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5DHVCl6M6fA5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import vgg19, resnet18, densenet161\n",
        "from torch.optim import AdamW, SGD, Adam\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "import clip\n",
        "\n",
        "\n",
        "from collections import OrderedDict\n",
        "from typing import Tuple, Union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Rsv3Y71S7G7x"
      },
      "outputs": [],
      "source": [
        "# Global Path Vairables\n",
        "ROOT_DIR =  \"drive/MyDrive/11877-AMMML/dataset/\"\n",
        "DATASET_DIR = ROOT_DIR + \"flickr8k/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pmhfHiHtFER0"
      },
      "outputs": [],
      "source": [
        "def get_all_files_from_dir(directory):\n",
        "    file_paths = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        file_paths += [os.path.join(root, x) for x in files]\n",
        "    return sorted(file_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218,
          "referenced_widgets": [
            "3934cdd6b34d444593541a2cf389eb57",
            "9d3895aeb90e490a8af02648a2088e81",
            "8eb081cb80f04806ac83b0dacdb083df",
            "74bab87498554151a9659d4f15f16248",
            "a2b3b5a6c5b541dbb43298f7007f3c97",
            "1166e05d7ff14a5b86ea72ea85cf6f7f",
            "f585f8cdb0d04e44a6a6ef6994381db3",
            "c454d96224ba4f0787dd373b13f981d5",
            "99a85e2621b64189a429813c3d10c32d",
            "c1c30f5d9f204826a8fab7402808adcd",
            "0a876377168d44b69f5cf3dc979a5a6e",
            "ef8b8ea52fd247a886a990fef502c370",
            "d98220236094488b8e7c36a5f027d9fa",
            "1da0a30884a04b7a91b8bf6c4b6ec142",
            "772703d32909498591d41961a896025c",
            "9ef82d40d5a445f4bd4bd4c97dc88bbf",
            "2a66240ba07346318f92be9f48adf535",
            "241e8184b4d74382b2230216b0ac4222",
            "c9d6b6c3697848fca58c6633c4b885e6",
            "d252bb8963ba4b04953b9faf69be82b8",
            "aeeb10a31c3945949ad2bbce3eed72ad",
            "b3cf8973e52847f7a4ff67827b6ce505",
            "37fc5493eead402abd7a671e486cf174",
            "fec8e87809fb40f688bfbd69f3476ce8",
            "2fdd4413dd3247cdaf9af3f0e811dc67",
            "67ab7152e0d44768bc41d33ad1474e20",
            "98baed37c74a4393bf0908deee20277b",
            "cb97140dff244f2284e0eef986740003",
            "8c626ff7af44483cad03a7c0ba3fd66d",
            "ddd198794f164c64940cc801544937a6",
            "719b323467f647c2880ab17299321ed5",
            "98882f069fda4a4fae37872b4b1e0ed6",
            "49978a2e95564ac4abe8bd1af7561216",
            "aeaa8939903f4cfc931463faa6bc8fb7",
            "dfab2bf396ca4589bfe1253d2325034a",
            "6d1fd27315e44df5b15a6d8d44e1ebec",
            "a0230542d09f4152971309b2f4424adc",
            "dfebf0c9dc6b4f57b3e51b494a051725",
            "6f63d5b3f97d489790235a3f0cce2ebf",
            "96fe4a8e611b444fbc246bebcc0a3f16",
            "d8d0de985f8c4c86935eb1a7fe865d6a",
            "454e92fc49b94218b508e75ff441502f",
            "6ad753e9e089447888fa724dbdce9c5b",
            "de5feb4246c9434b975a19d0d7910e7b"
          ]
        },
        "id": "avZcoj9zMloD",
        "outputId": "01029542-e184-42e2-95c3-61918d3e9331"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3934cdd6b34d444593541a2cf389eb57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef8b8ea52fd247a886a990fef502c370"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37fc5493eead402abd7a671e486cf174"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aeaa8939903f4cfc931463faa6bc8fb7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n",
        "\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = None\n",
        "        self.stride = stride\n",
        "\n",
        "        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
        "            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n",
        "            self.downsample = nn.Sequential(OrderedDict([\n",
        "                (\"-1\", nn.AvgPool2d(stride)),\n",
        "                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n",
        "                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        identity = x\n",
        "\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.avgpool(out)\n",
        "        out = self.bn3(self.conv3(out))\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class AttentionPool2d(nn.Module):\n",
        "    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n",
        "        super().__init__()\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC\n",
        "        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n",
        "        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n",
        "        x, _ = F.multi_head_attention_forward(\n",
        "            query=x, key=x, value=x,\n",
        "            embed_dim_to_check=x.shape[-1],\n",
        "            num_heads=self.num_heads,\n",
        "            q_proj_weight=self.q_proj.weight,\n",
        "            k_proj_weight=self.k_proj.weight,\n",
        "            v_proj_weight=self.v_proj.weight,\n",
        "            in_proj_weight=None,\n",
        "            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n",
        "            bias_k=None,\n",
        "            bias_v=None,\n",
        "            add_zero_attn=False,\n",
        "            dropout_p=0,\n",
        "            out_proj_weight=self.c_proj.weight,\n",
        "            out_proj_bias=self.c_proj.bias,\n",
        "            use_separate_proj_weight=True,\n",
        "            training=self.training,\n",
        "            need_weights=False\n",
        "        )\n",
        "\n",
        "        return x[0]\n",
        "\n",
        "\n",
        "class ModifiedResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A ResNet class that is similar to torchvision's but contains the following changes:\n",
        "    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n",
        "    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n",
        "    - The final pooling layer is a QKV attention instead of an average pool\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.input_resolution = input_resolution\n",
        "\n",
        "        # the 3-layer stem\n",
        "        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(width // 2)\n",
        "        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(width // 2)\n",
        "        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(width)\n",
        "        self.avgpool = nn.AvgPool2d(2)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # residual layers\n",
        "        self._inplanes = width  # this is a *mutable* variable used during construction\n",
        "        self.layer1 = self._make_layer(width, layers[0])\n",
        "        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n",
        "\n",
        "        embed_dim = width * 32  # the ResNet feature dimension\n",
        "        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)\n",
        "\n",
        "    def _make_layer(self, planes, blocks, stride=1):\n",
        "        layers = [Bottleneck(self._inplanes, planes, stride)]\n",
        "\n",
        "        self._inplanes = planes * Bottleneck.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(Bottleneck(self._inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        def stem(x):\n",
        "            for conv, bn in [(self.conv1, self.bn1), (self.conv2, self.bn2), (self.conv3, self.bn3)]:\n",
        "                x = self.relu(bn(conv(x)))\n",
        "            x = self.avgpool(x)\n",
        "            return x\n",
        "\n",
        "        x = x.type(self.conv1.weight.dtype)\n",
        "        x = stem(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.attnpool(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class LayerNorm(nn.LayerNorm):\n",
        "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        orig_type = x.dtype\n",
        "        ret = super().forward(x.type(torch.float32))\n",
        "        return ret.type(orig_type)\n",
        "\n",
        "\n",
        "class QuickGELU(nn.Module):\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return x * torch.sigmoid(1.702 * x)\n",
        "\n",
        "\n",
        "class ResidualAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
        "        self.ln_1 = LayerNorm(d_model)\n",
        "        self.mlp = nn.Sequential(OrderedDict([\n",
        "            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
        "            (\"gelu\", QuickGELU()),\n",
        "            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
        "        ]))\n",
        "        self.ln_2 = LayerNorm(d_model)\n",
        "        self.attn_mask = attn_mask\n",
        "\n",
        "    def attention(self, x: torch.Tensor):\n",
        "        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
        "        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = x + self.attention(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n",
        "        super().__init__()\n",
        "        self.width = width\n",
        "        self.layers = layers\n",
        "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.resblocks(x)\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):\n",
        "        super().__init__()\n",
        "        self.input_resolution = input_resolution\n",
        "        self.output_dim = output_dim\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n",
        "\n",
        "        scale = width ** -0.5\n",
        "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
        "        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n",
        "        self.ln_pre = LayerNorm(width)\n",
        "\n",
        "        self.transformer = Transformer(width, layers, heads)\n",
        "\n",
        "        self.ln_post = LayerNorm(width)\n",
        "        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.conv1(x)  # shape = [*, width, grid, grid]\n",
        "        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
        "        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
        "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
        "        x = x + self.positional_embedding.to(x.dtype)\n",
        "        x = self.ln_pre(x)\n",
        "\n",
        "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
        "\n",
        "        x = self.ln_post(x[:, 0, :])\n",
        "\n",
        "        if self.proj is not None:\n",
        "            x = x @ self.proj\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class CLIP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 embed_dim: int,\n",
        "                 # vision\n",
        "                 image_resolution: int,\n",
        "                 vision_layers: Union[Tuple[int, int, int, int], int],\n",
        "                 vision_width: int,\n",
        "                 vision_patch_size: int,\n",
        "                 # text\n",
        "                 context_length: int,\n",
        "                 vocab_size: int,\n",
        "                 transformer_width: int,\n",
        "                 transformer_heads: int,\n",
        "                 transformer_layers: int\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.context_length = context_length\n",
        "\n",
        "        if isinstance(vision_layers, (tuple, list)):\n",
        "            vision_heads = vision_width * 32 // 64\n",
        "            self.visual = ModifiedResNet(\n",
        "                layers=vision_layers,\n",
        "                output_dim=embed_dim,\n",
        "                heads=vision_heads,\n",
        "                input_resolution=image_resolution,\n",
        "                width=vision_width\n",
        "            )\n",
        "        else:\n",
        "            vision_heads = vision_width // 64\n",
        "            self.visual = VisionTransformer(\n",
        "                input_resolution=image_resolution,\n",
        "                patch_size=vision_patch_size,\n",
        "                width=vision_width,\n",
        "                layers=vision_layers,\n",
        "                heads=vision_heads,\n",
        "                output_dim=embed_dim\n",
        "            )\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            width=transformer_width,\n",
        "            layers=transformer_layers,\n",
        "            heads=transformer_heads,\n",
        "            attn_mask=self.build_attention_mask()\n",
        "        )\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
        "        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
        "        self.ln_final = LayerNorm(transformer_width)\n",
        "\n",
        "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
        "\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
        "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
        "\n",
        "        if isinstance(self.visual, ModifiedResNet):\n",
        "            if self.visual.attnpool is not None:\n",
        "                std = self.visual.attnpool.c_proj.in_features ** -0.5\n",
        "                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n",
        "                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n",
        "                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n",
        "                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n",
        "\n",
        "            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:\n",
        "                for name, param in resnet_block.named_parameters():\n",
        "                    if name.endswith(\"bn3.weight\"):\n",
        "                        nn.init.zeros_(param)\n",
        "\n",
        "        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
        "        attn_std = self.transformer.width ** -0.5\n",
        "        fc_std = (2 * self.transformer.width) ** -0.5\n",
        "        for block in self.transformer.resblocks:\n",
        "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
        "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
        "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
        "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
        "\n",
        "        if self.text_projection is not None:\n",
        "            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
        "\n",
        "    def build_attention_mask(self):\n",
        "        # lazily create causal attention mask, with full attention between the vision tokens\n",
        "        # pytorch uses additive attention mask; fill with -inf\n",
        "        mask = torch.empty(self.context_length, self.context_length)\n",
        "        mask.fill_(float(\"-inf\"))\n",
        "        mask.triu_(1)  # zero out the lower diagonal\n",
        "        return mask\n",
        "\n",
        "    @property\n",
        "    def dtype(self):\n",
        "        return self.visual.conv1.weight.dtype\n",
        "\n",
        "    def encode_image(self, image):\n",
        "        return self.visual(image.type(self.dtype))\n",
        "\n",
        "    def encode_text_for_explain(self, text):\n",
        "        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n",
        "\n",
        "        x = x + self.positional_embedding.type(self.dtype)\n",
        "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
        "        x = self.ln_final(x).type(self.dtype)\n",
        "\n",
        "        # # x.shape = [batch_size, n_ctx, transformer.width]\n",
        "        # # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "        # x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
        "        return x\n",
        "    \n",
        "    def encode_text(self, text):\n",
        "        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n",
        "\n",
        "        x = x + self.positional_embedding.type(self.dtype)\n",
        "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
        "        x = self.ln_final(x).type(self.dtype)\n",
        "\n",
        "        # # x.shape = [batch_size, n_ctx, transformer.width]\n",
        "        # # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
        "        return x\n",
        "\n",
        "    def forward(self, image, text):\n",
        "        image_features = self.encode_image(image)\n",
        "        text_features = self.encode_text_for_explain(text)\n",
        "\n",
        "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
        "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "        text_features = text_features[torch.arange(text_features.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
        "\n",
        "        # normalized features\n",
        "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
        "\n",
        "        # cosine similarity as logits\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
        "        logits_per_text = logits_per_image.t()\n",
        "\n",
        "        # shape = [global_batch_size, global_batch_size]\n",
        "        return logits_per_image, logits_per_text\n",
        "\n",
        "\n",
        "def convert_weights(model: nn.Module):\n",
        "    \"\"\"Convert applicable model parameters to fp16\"\"\"\n",
        "\n",
        "    def _convert_weights_to_fp16(l):\n",
        "        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n",
        "            # l.weight.data = l.weight.data.half()\n",
        "            l.weight.data = l.weight.data.float()\n",
        "            if l.bias is not None:\n",
        "                # l.bias.data = l.bias.data.half()\n",
        "                l.bias.data = l.bias.data.float()\n",
        "\n",
        "        if isinstance(l, nn.MultiheadAttention):\n",
        "            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n",
        "                tensor = getattr(l, attr)\n",
        "                if tensor is not None:\n",
        "                    # tensor.data = tensor.data.half()\n",
        "                    tensor.data = tensor.data.float()\n",
        "\n",
        "        for name in [\"text_projection\", \"proj\"]:\n",
        "            if hasattr(l, name):\n",
        "                attr = getattr(l, name)\n",
        "                if attr is not None:\n",
        "                    # attr.data = attr.data.half()\n",
        "                    attr.data = attr.data.float()\n",
        "\n",
        "    model.apply(_convert_weights_to_fp16)\n",
        "\n",
        "\n",
        "def build_model(state_dict: dict):\n",
        "    vit = \"visual.proj\" in state_dict\n",
        "\n",
        "    if vit:\n",
        "        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n",
        "        vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n",
        "        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n",
        "        grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
        "        image_resolution = vision_patch_size * grid_size\n",
        "    else:\n",
        "        counts: list = [len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]\n",
        "        vision_layers = tuple(counts)\n",
        "        vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n",
        "        output_width = round((state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
        "        vision_patch_size = None\n",
        "        assert output_width ** 2 + 1 == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n",
        "        image_resolution = output_width * 32\n",
        "\n",
        "    embed_dim = state_dict[\"text_projection\"].shape[1]\n",
        "    context_length = state_dict[\"positional_embedding\"].shape[0]\n",
        "    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n",
        "    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n",
        "    transformer_heads = transformer_width // 64\n",
        "    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"transformer.resblocks\")))\n",
        "\n",
        "    model = CLIP(\n",
        "        embed_dim,\n",
        "        image_resolution, vision_layers, vision_width, vision_patch_size,\n",
        "        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers\n",
        "    )\n",
        "\n",
        "    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n",
        "        if key in state_dict:\n",
        "            del state_dict[key]\n",
        "\n",
        "    convert_weights(model)\n",
        "    model.load_state_dict(state_dict)\n",
        "    return model.eval()"
      ],
      "metadata": {
        "id": "DxpNfR3AJ_A8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, preprocess = clip.load(\"ViT-B/32\")\n",
        "checkpoint = torch.jit.load(ROOT_DIR + \"ViT-B-32.pt\")\n",
        "state_dict = checkpoint.state_dict()\n",
        "clip_model = build_model(state_dict)"
      ],
      "metadata": {
        "id": "EQlCwMokKFkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8e1f591-f6e3-41e9-c5d7-eb07ee7048f7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:01<00:00, 314MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "yTMB6rbM4ba1"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdHMYskj6v3U"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "Z5Wypx7ZEUEh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e830866-34f6-4e59-ad00-da37b9923aef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 600 images in the dataset\n",
            "There are 3000 captions in the dataset\n",
            "Notice that one image might map to many captions here\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(DATASET_DIR + \"captions.txt\").head(3000)\n",
        "print(\"There are\", len(df.image.unique()), \"images in the dataset\")\n",
        "print(\"There are\", len(df.caption.unique()), \"captions in the dataset\")\n",
        "print(\"Notice that one image might map to many captions here\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ctuz73VJMmur",
        "outputId": "fa5f002d-cd02-43ae-dc3d-7a452962c94c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size 6000\n"
          ]
        }
      ],
      "source": [
        "image_paths = df.image.tolist()\n",
        "texts = df.caption.tolist()\n",
        "all_positive_pairs = [(image, text, 1) for image, text in zip(image_paths, texts)]\n",
        "\n",
        "# negative sampling: random sample each, random shuffle, combine\n",
        "negative_size = int(df.size * 0.5)\n",
        "# sampled_images = df.image.unique().tolist()\n",
        "sampled_images = random.sample(image_paths, negative_size)\n",
        "random.shuffle(sampled_images[::-1])\n",
        "sampled_texts = random.sample(texts, negative_size)\n",
        "random.shuffle(sampled_texts[::-1])\n",
        "negative_pairs = []\n",
        "for image, text in zip(sampled_images, sampled_texts):\n",
        "  if (image, text) not in all_positive_pairs:\n",
        "    # negative_pairs.append((image, \"negative text\", 0))\n",
        "    negative_pairs.append((image, text, 0))\n",
        "\n",
        "positive_pairs = random.sample(all_positive_pairs, len(negative_pairs))\n",
        "all_pairs = positive_pairs + negative_pairs\n",
        "# all_pairs = positive_pairs\n",
        "random.shuffle(all_pairs)\n",
        "print(\"Training set size\", len(all_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "_jWCmRwdZGDi"
      },
      "outputs": [],
      "source": [
        "image_paths = [item[0] for item in all_pairs]\n",
        "texts = [item[1] for item in all_pairs]\n",
        "labels = [item[2] for item in all_pairs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "au4XmWUrYVkL"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None, is_clip=False):\n",
        "        super(ImageDataset, self).__init__()\n",
        "        self.image_paths = [DATASET_DIR + \"Images/\" + x for x in image_paths]\n",
        "        self.transform = transform\n",
        "        self.idx_to_path = {i: image_path for i, image_path in enumerate(self.image_paths)}\n",
        "        self.path_to_idx = {image_path: i for i, image_path in enumerate(self.image_paths)}\n",
        "        self.is_clip = is_clip\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        image_filepath = self.idx_to_path[idx]\n",
        "        if not self.is_clip:\n",
        "          image = Image.open(image_filepath)\n",
        "          if self.transform:\n",
        "            try:\n",
        "              image = self.transform(image)\n",
        "            except:\n",
        "              print(image_filepath)\n",
        "        else:\n",
        "          image = preprocess(Image.open(image_filepath))\n",
        "        return image\n",
        "    \n",
        "    def get_by_path(self, path):\n",
        "      image, _ =  self.__getitem__(self.path_to_idx[path])\n",
        "      image = image[None, :]\n",
        "      return image\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "  def __init__(self, texts, targets, tokenizer, max_len, is_clip=False):\n",
        "    self.texts = texts\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    self.is_clip = is_clip\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    text = str(self.texts[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    # if target == 1:\n",
        "    #   target = np.array([1, 0])\n",
        "    # else:\n",
        "    #   target = np.array([0, 1])\n",
        "\n",
        "    if self.is_clip:\n",
        "      encoding = clip.tokenize(text).flatten()\n",
        "      return {\n",
        "          'text': text,\n",
        "          'clip_encoding': encoding,\n",
        "          'targets': torch.tensor(target, dtype=torch.long)\n",
        "      }\n",
        "    else:\n",
        "      encoding = self.tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_len,\n",
        "        return_token_type_ids=False,\n",
        "        # pad_to_max_length=True,\n",
        "        padding=\"max_length\",\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "      )\n",
        "\n",
        "      return {\n",
        "        'text': text,\n",
        "        'input_ids': encoding['input_ids'].flatten(),\n",
        "        'attention_mask': encoding['attention_mask'].flatten(),\n",
        "        'targets': torch.tensor(target, dtype=torch.long)\n",
        "      }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "_iVkgFcKYVmu"
      },
      "outputs": [],
      "source": [
        "# use the ImageNet transformation\n",
        "imagenet_transform = transforms.Compose([transforms.Resize((256, 256)), \n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                                ])\n",
        "\n",
        "# Already double checked the order of ids in dataset and loader objects are as expected\n",
        "image_dataset = ImageDataset(image_paths, imagenet_transform, is_clip=True)\n",
        "image_dataloader = DataLoader(dataset=image_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "R0cN4Z-D8WCr"
      },
      "outputs": [],
      "source": [
        "text_dataset = TextDataset(\n",
        "    texts=texts,\n",
        "    targets=labels,\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=160,\n",
        "    is_clip=True\n",
        "  )\n",
        "\n",
        "text_dataloader = DataLoader(dataset=text_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "3VrRqRKW_2lt"
      },
      "outputs": [],
      "source": [
        "assert len(image_dataloader) == len(text_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAmDixr5FE8_"
      },
      "source": [
        "# Model To Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "RrjSRdqf0cGn"
      },
      "outputs": [],
      "source": [
        "class MultimodalClipClassifier(nn.Module):\n",
        "  def __init__(self, clip_model, n_classes):\n",
        "    super(MultimodalClipClassifier, self).__init__()\n",
        "    self.clip_model = clip_model\n",
        "    # self.drop = nn.Dropout(p=0.3)\n",
        "    self.combined_fc1 = nn.Linear(512*1, 256)\n",
        "    self.combined_fc2 = nn.Linear(256, 128)\n",
        "    self.output_fc = nn.Linear(128, n_classes)\n",
        "    self.text_projection = clip_model.text_projection\n",
        "  \n",
        "  def forward(self, image, text):\n",
        "    image_emb = self.clip_model.encode_image(image)\n",
        "    text_emb = self.clip_model.encode_text_for_explain(text)\n",
        "    text_emb = text_emb[torch.arange(text_emb.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
        "\n",
        "    # all_emb = torch.cat((text_emb, image_emb, text_emb * image_emb), 1)\n",
        "    all_emb = text_emb * image_emb\n",
        "    x_comb = F.relu(self.combined_fc1(all_emb))\n",
        "    x_comb = F.relu(self.combined_fc2(x_comb))\n",
        "    out = self.output_fc(x_comb)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXPRTxo5Nwvs",
        "outputId": "3bd42ae4-6fba-4e8a-a128-e1f5fc2163f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently using device: cuda\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Currently using device: {}\\n\".format(DEVICE))\n",
        "\n",
        "\n",
        "model = MultimodalClipClassifier(clip_model=clip_model, n_classes=1)\n",
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # let's do a simple test on clip's encoding capability!\n",
        "# index = random.sample(range(len(all_pairs)), 1)[0]\n",
        "# print(\"Index:\", index)\n",
        "# cv2_image = cv2.imread(DATASET_DIR + \"Images/\" + all_pairs[index][0])\n",
        "# # print(cv2_image.shape)\n",
        "# print(\"Caption:\", all_pairs[index][1])\n",
        "# print(\"Label:\", all_pairs[index][2])\n",
        "# plt.imshow(cv2_image)\n",
        "# plt.show()\n",
        "\n",
        "# image_encoding = model.clip_model.encode_image(preprocess(Image.open(DATASET_DIR + \"Images/\" + all_pairs[index][0]))[None, :]\n",
        "#                                                .to(DEVICE)).cpu().detach().numpy().flatten()\n",
        "# text_encoding = model.clip_model.encode_text(clip.tokenize(all_pairs[index][1]).to(DEVICE)).cpu().detach().numpy().flatten()\n",
        "\n",
        "# cos_sim = np.dot(image_encoding, text_encoding)/(np.linalg.norm(image_encoding) * np.linalg.norm(text_encoding))\n",
        "# cos_sim"
      ],
      "metadata": {
        "id": "jRXT2aRshFHf"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "AKlDsbOqSvDY",
        "outputId": "7731d7f8-a638-4199-cbed-a5416e9a4432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Started...\n",
            "Epoch: 1\n",
            "Train accuracy: 0.90625\n",
            "Train loss 0.2553960626865638\n",
            "\n",
            "Epoch: 2\n",
            "Train accuracy: 0.9798869680851063\n",
            "Train loss 0.06591971946305576\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-18006ca5ff8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_encodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;31m# loss = criterion(torch.round(torch.sigmoid(output.squeeze())), targets.float())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \"If you need gradients in your forward method, consider using autograd.grad instead.\")\n\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mset_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mparameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \"\"\"\n\u001b[0;32m-> 1535\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1536\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, prefix, recurse)\u001b[0m\n\u001b[1;32m   1559\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m             prefix=prefix, recurse=recurse)\n\u001b[0;32m-> 1561\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1562\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_named_members\u001b[0;34m(self, get_members_fn, prefix, recurse)\u001b[0m\n\u001b[1;32m   1508\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1510\u001b[0;31m                 \u001b[0mmemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1511\u001b[0m                 \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodule_prefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_unary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__hash__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"Training Started...\")\n",
        "criterion = nn.BCEWithLogitsLoss() # this means the sigmoid is INCORPORATED into the loss!!\n",
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Fine tune CLIP or use original CLIP?\n",
        "for p in model.clip_model.parameters():\n",
        "    p.requires_grad = False\n",
        "for p in model.clip_model.get_submodule(\"token_embedding\").parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# Scale gradients to use fp16 training\n",
        "scaler = GradScaler()\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  total_acc_train = 0\n",
        "  total_loss_train = 0\n",
        "\n",
        "  for text_features, image_features in zip(text_dataloader, image_dataloader):\n",
        "    with autocast():\n",
        "      text_encodings = text_features[\"clip_encoding\"].to(DEVICE)\n",
        "      targets = text_features[\"targets\"].to(DEVICE)\n",
        "      image_features = image_features.to(DEVICE)\n",
        "\n",
        "      model.zero_grad()\n",
        "      output = model(image_features, text_encodings)\n",
        "      # loss = criterion(torch.round(torch.sigmoid(output.squeeze())), targets.float())\n",
        "      loss = criterion(output.squeeze(), targets.float())\n",
        "    \n",
        "    # scaler.scale(loss).backward() \n",
        "    loss.backward()\n",
        "    # scaler.unscale_(optimizer)\n",
        "    # nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "    # scaler.step(optimizer) \n",
        "    optimizer.step()\n",
        "    # scaler.update()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      acc = torch.abs(torch.round(torch.sigmoid(output.squeeze())) - targets.float()).view(-1)\n",
        "      acc = (1. - acc.sum() / acc.size()[0])\n",
        "      total_acc_train += acc.item()\n",
        "      total_loss_train += loss.item()\n",
        "    \n",
        "  train_acc = total_acc_train / len(text_dataloader)\n",
        "  train_loss = total_loss_train / len(text_dataloader)\n",
        "  print(\"Epoch:\", epoch + 1)\n",
        "  print(\"Train accuracy:\", train_acc)\n",
        "  print(\"Train loss\", train_loss)\n",
        "  print()\n",
        "\n",
        "torch.save(model.state_dict(), ROOT_DIR + \"early_funsion.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNahzKPDFx_l"
      },
      "source": [
        "# Explainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CgUGUZ3HAIDG"
      },
      "outputs": [],
      "source": [
        "class MultimodalExplainer(nn.Module):\n",
        "  # This somehow only works for batch = 1, but this is enough for a case-by-case explainer for our current need\n",
        "  def __init__(self, n_classes):\n",
        "    super(MultimodalExplainer, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    self.vgg = vgg19(pretrained=True)\n",
        "    self.features_conv = self.vgg.features[:36]\n",
        "    self.features_final = self.vgg.classifier[:-1]\n",
        "    self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.combined_fc1 = nn.Linear(self.bert.config.hidden_size + 4096, 256)\n",
        "    self.combined_fc_final = nn.Linear(256, n_classes)\n",
        "\n",
        "    self.raw_image_input = None\n",
        "    self.raw_language_input = None\n",
        "    \n",
        "    # placeholder for the gradients\n",
        "    self.vision_gradients = None\n",
        "    self.language_gradients = None\n",
        "\n",
        "  # hook for the gradients of the activations\n",
        "  def vision_activations_hook(self, grad):\n",
        "      self.vision_gradients = grad\n",
        "  \n",
        "  def language_activations_hook(self, grad):\n",
        "      self.language_gradients = grad\n",
        "\n",
        "  def forward(self, x, input_ids, attention_mask):\n",
        "    self.raw_image_input = x.clone().detach().cpu().numpy()\n",
        "    x = self.features_conv(x)\n",
        "    h_vision = x.register_hook(self.vision_activations_hook)\n",
        "    x = self.max_pool(x)\n",
        "    x = x.view((x.size(0), -1))\n",
        "    x = self.features_final(x)\n",
        "    \n",
        "    self.raw_language_input = input_ids.clone().detach().cpu().numpy()\n",
        "    for module_pos, module in self.bert._modules.items():\n",
        "      if module_pos == \"embeddings\":\n",
        "        y = module(input_ids=input_ids)\n",
        "        h_language = y.register_hook(self.language_activations_hook)\n",
        "      elif module_pos == \"encoder\":\n",
        "        y = module(hidden_states=y, attention_mask=attention_mask, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None).last_hidden_state\n",
        "      else:\n",
        "        pooled_output = module(hidden_states=y)\n",
        "        last_hidden_state = y\n",
        "    \n",
        "    combined = torch.cat((x, pooled_output), 1)\n",
        "    combined = F.relu(self.combined_fc1(combined))\n",
        "    combined = F.relu(self.combined_fc_final(combined))\n",
        "    return combined\n",
        "  \n",
        "  def get_vision_gradient(self):\n",
        "    return self.vision_gradients\n",
        "\n",
        "  def get_language_gradient(self):\n",
        "    return self.language_gradients\n",
        "  \n",
        "   # method for the activation exctraction\n",
        "  def get_vision_activations(self, x):\n",
        "    return self.features_conv(x).detach().cpu().numpy()\n",
        "  \n",
        "  def get_language_activations(self):\n",
        "    return self.raw_language_input\n",
        "\n",
        "  def get_raw_vision_input(self):\n",
        "    return self.raw_image_input\n",
        "\n",
        "  def get_raw_language_input(self):\n",
        "    return self.raw_language_input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jEC7hNr-AIFT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917,
          "referenced_widgets": [
            "91d83e4b38f04444a781a752fcb0c8f5",
            "327e975dfc4d490ea345a5210da3002f",
            "bd8e8e3d30324b0f943bc9fd3edda382",
            "3337ee1c958e4f648c873ee7c682c805",
            "0aec86669d214c9a873594c46e812655",
            "6081a9201823415fa9c174f06e722235",
            "fc39b1dd38724369b6e73379f31af728",
            "9482452bc0b14a6b8fd08afe261b07d4",
            "ca7f2a84a5774e3a9342375da0454011",
            "6abc7243ba0045bf98ae803a347e0c5f",
            "106c7ea9693546ef9b90effd81bbbdfd"
          ]
        },
        "outputId": "32b56314-ca72-4590-87af-25e3d870e7eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91d83e4b38f04444a781a752fcb0c8f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-689b9539c9e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_explainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultimodalExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_explainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"early_funsion.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel_explainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_explainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_explainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_explainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1498\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MultimodalExplainer:\n\tMissing key(s) in state_dict: \"bert.embeddings.position_ids\", \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer....\n\tUnexpected key(s) in state_dict: \"text_projection\", \"clip_model.positional_embedding\", \"clip_model.text_projection\", \"clip_model.logit_scale\", \"clip_model.visual.class_embedding\", \"clip_model.visual.positional_embedding\", \"clip_model.visual.proj\", \"clip_model.visual.conv1.weight\", \"clip_model.visual.ln_pre.weight\", \"clip_model.visual.ln_pre.bias\", \"clip_model.visual.transformer.resblocks.0.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.0.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.0.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.0.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.0.ln_1.weight\", \"clip_model.visual.transformer.resblocks.0.ln_1.bias\", \"clip_model.visual.transformer.resblocks.0.mlp.c_fc.weight\", \"clip_model.visual.transformer.resblocks.0.mlp.c_fc.bias\", \"clip_model.visual.transformer.resblocks.0.mlp.c_proj.weight\", \"clip_model.visual.transformer.resblocks.0.mlp.c_proj.bias\", \"clip_model.visual.transformer.resblocks.0.ln_2.weight\", \"clip_model.visual.transformer.resblocks.0.ln_2.bias\", \"clip_model.visual.transformer.resblocks.1.attn.in_proj_weight\", \"clip_model.visual.transformer.resblocks.1.attn.in_proj_bias\", \"clip_model.visual.transformer.resblocks.1.attn.out_proj.weight\", \"clip_model.visual.transformer.resblocks.1.attn.out_proj.bias\", \"clip_model.visual.transformer.resblocks.1.ln_1.weight\", \"clip_model.visual.transformer.resblocks.1.ln_1.bias\", \"clip_model.visual.transformer.resblocks.1.mlp.c_f...\n\tsize mismatch for combined_fc1.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 4864])."
          ]
        }
      ],
      "source": [
        "model_explainer = MultimodalExplainer(n_classes=2)\n",
        "model_explainer.load_state_dict(torch.load(ROOT_DIR + \"early_funsion.pt\"))\n",
        "model_explainer = model_explainer.to(DEVICE)\n",
        "model_explainer = model_explainer.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUMHWOBCBc3U"
      },
      "outputs": [],
      "source": [
        "# Update the three fields\n",
        "index = random.sample(range(len(all_pairs)), 1)[0]\n",
        "sample_image_path = DATASET_DIR + \"Images/\" + all_pairs[index][0]\n",
        "sample_text = all_pairs[index][1]\n",
        "sample_label = all_pairs[index][2]\n",
        "\n",
        "sample_image = Image.open(sample_image_path)\n",
        "plt.imshow(sample_image)\n",
        "sample_image = imagenet_transform(sample_image)\n",
        "sample_image = sample_image[None, :]\n",
        "\n",
        "\n",
        "sample_encoding = tokenizer.encode_plus(\n",
        "      sample_text,\n",
        "      add_special_tokens=True,\n",
        "      max_length=160,\n",
        "      return_token_type_ids=False,\n",
        "      # pad_to_max_length=True,\n",
        "      padding=\"max_length\",\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "sample_image = sample_image.to(DEVICE)\n",
        "sample_encoding = sample_encoding.to(DEVICE)\n",
        "\n",
        "print(sample_text)\n",
        "print(sample_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynpv_5NHF9sQ"
      },
      "outputs": [],
      "source": [
        "# outputs = model_explainer(sample_image, sample_encoding[\"input_ids\"], None)\n",
        "outputs = model_explainer(sample_image, sample_encoding[\"input_ids\"], sample_encoding[\"attention_mask\"])\n",
        "probs = F.softmax(outputs, dim=1)\n",
        "_, preds = torch.max(outputs, dim=1)\n",
        "pred_index = preds.detach().cpu().numpy()[0]\n",
        "outputs[:, pred_index].backward()\n",
        "print(probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Bscwe3oMQqY"
      },
      "outputs": [],
      "source": [
        "vision_gradients = model_explainer.get_vision_gradient()\n",
        "# pool the gradients across the channels, so that each channel have one gradient value\n",
        "pooled_vision_gradients = torch.mean(vision_gradients, dim=[0, 2, 3])\n",
        "pooled_vision_gradients_np = pooled_vision_gradients.detach().cpu().numpy()\n",
        "\n",
        "# get the activations of the last convolutional layer\n",
        "raw_vision_activations = model_explainer.get_vision_activations(sample_image)\n",
        "vision_activations = np.zeros(raw_vision_activations.shape)\n",
        "\n",
        "# weight the channels by corresponding gradients\n",
        "for i in range(vision_activations[0].shape[0]):\n",
        "  vision_activations[:, i, :, :] = raw_vision_activations[:, i, :, :] * pooled_vision_gradients_np[i]\n",
        "    \n",
        "# average the channels of the activations\n",
        "vision_heatmap = torch.mean(torch.from_numpy(vision_activations), axis=1).squeeze()\n",
        "\n",
        "# relu on top of the heatmap\n",
        "# expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
        "vision_heatmap = np.maximum(vision_heatmap, 0)\n",
        "\n",
        "# normalize the heatmap\n",
        "vision_heatmap /= torch.max(vision_heatmap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEzK31zNNTAC"
      },
      "outputs": [],
      "source": [
        "# Plot heatmap together with the image from ORIGINAL RESOLUTION\n",
        "plot_image = cv2.imread(sample_image_path)\n",
        "updated_vision_heatmap = cv2.resize(np.array(vision_heatmap), (plot_image.shape[1], plot_image.shape[0]))\n",
        "updated_vision_heatmap = np.uint8(255 * updated_vision_heatmap)\n",
        "updated_vision_heatmap = cv2.applyColorMap(updated_vision_heatmap, cv2.COLORMAP_JET)\n",
        "superimposed_img = updated_vision_heatmap * 0.5 + plot_image\n",
        "\n",
        "f = plt.figure()\n",
        "f.add_subplot(1, 3, 1)\n",
        "plt.imshow(plot_image)\n",
        "f.add_subplot(1, 3, 2)\n",
        "plt.imshow(vision_heatmap)\n",
        "f.add_subplot(1, 3, 3)\n",
        "plt.imshow(superimposed_img / np.max(superimposed_img))\n",
        "plt.show(block=True)\n",
        "print(sample_text)\n",
        "if sample_label == 1:\n",
        "  print(\"Ground Truth: Image and Caption Matched\")\n",
        "else:\n",
        "  print(\"Ground Truth: Image and Caption NOT Matched\")\n",
        "if pred_index == 0:\n",
        "  print(\"Predicted: Image and Caption Matched\")\n",
        "else:\n",
        "  print(\"Predicted: Image and Caption NOT Matched\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_UcpfFZOrCw"
      },
      "outputs": [],
      "source": [
        "language_gradients = model_explainer.get_language_gradient()\n",
        "pooled_language_gradients = torch.mean(language_gradients, dim=2)\n",
        "pooled_language_gradients_np = pooled_language_gradients.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byR8oomJCgDT"
      },
      "outputs": [],
      "source": [
        "np.argmax(pooled_language_gradients_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohnHsmWdOeaa"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(sample_encoding[\"input_ids\"][0])\n",
        "print(tokens)\n",
        "print(tokens[np.argmax(pooled_language_gradients_np) - 2 : np.argmax(pooled_language_gradients_np) + 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWHcHkJ7jPn3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "sNahzKPDFx_l"
      ],
      "machine_shape": "hm",
      "name": "multimodal_gradcam_CLIP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3934cdd6b34d444593541a2cf389eb57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d3895aeb90e490a8af02648a2088e81",
              "IPY_MODEL_8eb081cb80f04806ac83b0dacdb083df",
              "IPY_MODEL_74bab87498554151a9659d4f15f16248"
            ],
            "layout": "IPY_MODEL_a2b3b5a6c5b541dbb43298f7007f3c97"
          }
        },
        "9d3895aeb90e490a8af02648a2088e81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1166e05d7ff14a5b86ea72ea85cf6f7f",
            "placeholder": "​",
            "style": "IPY_MODEL_f585f8cdb0d04e44a6a6ef6994381db3",
            "value": "Downloading: 100%"
          }
        },
        "8eb081cb80f04806ac83b0dacdb083df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c454d96224ba4f0787dd373b13f981d5",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99a85e2621b64189a429813c3d10c32d",
            "value": 213450
          }
        },
        "74bab87498554151a9659d4f15f16248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1c30f5d9f204826a8fab7402808adcd",
            "placeholder": "​",
            "style": "IPY_MODEL_0a876377168d44b69f5cf3dc979a5a6e",
            "value": " 208k/208k [00:00&lt;00:00, 275kB/s]"
          }
        },
        "a2b3b5a6c5b541dbb43298f7007f3c97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1166e05d7ff14a5b86ea72ea85cf6f7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f585f8cdb0d04e44a6a6ef6994381db3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c454d96224ba4f0787dd373b13f981d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99a85e2621b64189a429813c3d10c32d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1c30f5d9f204826a8fab7402808adcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a876377168d44b69f5cf3dc979a5a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef8b8ea52fd247a886a990fef502c370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d98220236094488b8e7c36a5f027d9fa",
              "IPY_MODEL_1da0a30884a04b7a91b8bf6c4b6ec142",
              "IPY_MODEL_772703d32909498591d41961a896025c"
            ],
            "layout": "IPY_MODEL_9ef82d40d5a445f4bd4bd4c97dc88bbf"
          }
        },
        "d98220236094488b8e7c36a5f027d9fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a66240ba07346318f92be9f48adf535",
            "placeholder": "​",
            "style": "IPY_MODEL_241e8184b4d74382b2230216b0ac4222",
            "value": "Downloading: 100%"
          }
        },
        "1da0a30884a04b7a91b8bf6c4b6ec142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9d6b6c3697848fca58c6633c4b885e6",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d252bb8963ba4b04953b9faf69be82b8",
            "value": 29
          }
        },
        "772703d32909498591d41961a896025c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aeeb10a31c3945949ad2bbce3eed72ad",
            "placeholder": "​",
            "style": "IPY_MODEL_b3cf8973e52847f7a4ff67827b6ce505",
            "value": " 29.0/29.0 [00:00&lt;00:00, 542B/s]"
          }
        },
        "9ef82d40d5a445f4bd4bd4c97dc88bbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a66240ba07346318f92be9f48adf535": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "241e8184b4d74382b2230216b0ac4222": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9d6b6c3697848fca58c6633c4b885e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d252bb8963ba4b04953b9faf69be82b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aeeb10a31c3945949ad2bbce3eed72ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3cf8973e52847f7a4ff67827b6ce505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37fc5493eead402abd7a671e486cf174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fec8e87809fb40f688bfbd69f3476ce8",
              "IPY_MODEL_2fdd4413dd3247cdaf9af3f0e811dc67",
              "IPY_MODEL_67ab7152e0d44768bc41d33ad1474e20"
            ],
            "layout": "IPY_MODEL_98baed37c74a4393bf0908deee20277b"
          }
        },
        "fec8e87809fb40f688bfbd69f3476ce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb97140dff244f2284e0eef986740003",
            "placeholder": "​",
            "style": "IPY_MODEL_8c626ff7af44483cad03a7c0ba3fd66d",
            "value": "Downloading: 100%"
          }
        },
        "2fdd4413dd3247cdaf9af3f0e811dc67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddd198794f164c64940cc801544937a6",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_719b323467f647c2880ab17299321ed5",
            "value": 570
          }
        },
        "67ab7152e0d44768bc41d33ad1474e20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98882f069fda4a4fae37872b4b1e0ed6",
            "placeholder": "​",
            "style": "IPY_MODEL_49978a2e95564ac4abe8bd1af7561216",
            "value": " 570/570 [00:00&lt;00:00, 9.01kB/s]"
          }
        },
        "98baed37c74a4393bf0908deee20277b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb97140dff244f2284e0eef986740003": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c626ff7af44483cad03a7c0ba3fd66d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ddd198794f164c64940cc801544937a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "719b323467f647c2880ab17299321ed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98882f069fda4a4fae37872b4b1e0ed6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49978a2e95564ac4abe8bd1af7561216": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aeaa8939903f4cfc931463faa6bc8fb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dfab2bf396ca4589bfe1253d2325034a",
              "IPY_MODEL_6d1fd27315e44df5b15a6d8d44e1ebec",
              "IPY_MODEL_a0230542d09f4152971309b2f4424adc"
            ],
            "layout": "IPY_MODEL_dfebf0c9dc6b4f57b3e51b494a051725"
          }
        },
        "dfab2bf396ca4589bfe1253d2325034a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f63d5b3f97d489790235a3f0cce2ebf",
            "placeholder": "​",
            "style": "IPY_MODEL_96fe4a8e611b444fbc246bebcc0a3f16",
            "value": "Downloading: 100%"
          }
        },
        "6d1fd27315e44df5b15a6d8d44e1ebec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8d0de985f8c4c86935eb1a7fe865d6a",
            "max": 435779157,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_454e92fc49b94218b508e75ff441502f",
            "value": 435779157
          }
        },
        "a0230542d09f4152971309b2f4424adc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ad753e9e089447888fa724dbdce9c5b",
            "placeholder": "​",
            "style": "IPY_MODEL_de5feb4246c9434b975a19d0d7910e7b",
            "value": " 416M/416M [00:06&lt;00:00, 69.0MB/s]"
          }
        },
        "dfebf0c9dc6b4f57b3e51b494a051725": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f63d5b3f97d489790235a3f0cce2ebf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96fe4a8e611b444fbc246bebcc0a3f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8d0de985f8c4c86935eb1a7fe865d6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "454e92fc49b94218b508e75ff441502f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ad753e9e089447888fa724dbdce9c5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de5feb4246c9434b975a19d0d7910e7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91d83e4b38f04444a781a752fcb0c8f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_327e975dfc4d490ea345a5210da3002f",
              "IPY_MODEL_bd8e8e3d30324b0f943bc9fd3edda382",
              "IPY_MODEL_3337ee1c958e4f648c873ee7c682c805"
            ],
            "layout": "IPY_MODEL_0aec86669d214c9a873594c46e812655"
          }
        },
        "327e975dfc4d490ea345a5210da3002f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6081a9201823415fa9c174f06e722235",
            "placeholder": "​",
            "style": "IPY_MODEL_fc39b1dd38724369b6e73379f31af728",
            "value": "100%"
          }
        },
        "bd8e8e3d30324b0f943bc9fd3edda382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9482452bc0b14a6b8fd08afe261b07d4",
            "max": 574673361,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca7f2a84a5774e3a9342375da0454011",
            "value": 574673361
          }
        },
        "3337ee1c958e4f648c873ee7c682c805": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6abc7243ba0045bf98ae803a347e0c5f",
            "placeholder": "​",
            "style": "IPY_MODEL_106c7ea9693546ef9b90effd81bbbdfd",
            "value": " 548M/548M [00:08&lt;00:00, 70.5MB/s]"
          }
        },
        "0aec86669d214c9a873594c46e812655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6081a9201823415fa9c174f06e722235": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc39b1dd38724369b6e73379f31af728": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9482452bc0b14a6b8fd08afe261b07d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca7f2a84a5774e3a9342375da0454011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6abc7243ba0045bf98ae803a347e0c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "106c7ea9693546ef9b90effd81bbbdfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}