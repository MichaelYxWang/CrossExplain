{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multimodal-gradcam-mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6TYShLC5ydW",
        "outputId": "744c4ea5-3fbc-4620-df44-08bd9b30a196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 32.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 34.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 32.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Create multmiodal dataloader for this\n",
        "2. Create a CNN-LSTM that can train and inference\n",
        "3. Add gradcam and all other stuff"
      ],
      "metadata": {
        "id": "u9sN9-Z4s8w-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import vgg19, resnet18, densenet161\n",
        "\n",
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from torch.optim import AdamW, SGD, Adam\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import pickle"
      ],
      "metadata": {
        "id": "6sV60oWZ6DRC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Path Vairables\n",
        "ROOT_DIR =  \"drive/MyDrive/11877-AMMML/dataset/\"\n",
        "DATASET_DIR = ROOT_DIR"
      ],
      "metadata": {
        "id": "ynhiZiXF6MSf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "LEARNING_RATE = 1e-4 # 0.001\n",
        "HIDDEN_DIM = 64 # 64\n",
        "N_LAYERS = 8 # 2\n",
        "EPOCHS = 15\n",
        "CLIP = 5\n",
        "DROPOUT = 0.1\n",
        "BATCH_SIZE = 4"
      ],
      "metadata": {
        "id": "dy_UjPCWlvZs"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pickle.load(open(DATASET_DIR + 'collaged_MNIST.p','rb'))"
      ],
      "metadata": {
        "id": "vnUa92qa6Wy-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 2\n",
        "image_bytes = dataset[\"image\"][index]\n",
        "plt.imshow(image_bytes, cmap=plt.cm.gray)\n",
        "print(\"Image label is\", dataset[\"label\"][index])\n",
        "print(\"Image text is\", dataset[\"text\"][index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "rgSkfVVD66Oj",
        "outputId": "ce296cfc-ffe9-45ce-9a09-6fd798dedb45"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image label is 0.0\n",
            "Image text is [4 0 0 5]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalMnistDataset(Dataset):\n",
        "  \"\"\"Dataset containing pairs of MNIST collaged images and text descriptions.\"\"\"\n",
        "  def __init__(self, images, texts, labels):\n",
        "    super(MultimodalMnistDataset, self).__init__()\n",
        "    self.images = [np.expand_dims(img.astype(np.float32) / 255, 0) for img in images]\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.images[idx], self.texts[idx], self.labels[idx]\n"
      ],
      "metadata": {
        "id": "19TkLQizv0Hd"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# texts = [\" \".join(list(map(str, x.tolist()))) for x in dataset[\"text\"]]\n",
        "# tokenizer = Tokenizer()\n",
        "# tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# tokenized_texts = tokenizer.texts_to_sequences(texts)\n",
        "# tokenized_texts = pad_sequences(tokenized_texts, maxlen=4)\n",
        "mnist_dataset = MultimodalMnistDataset(dataset[\"image\"],\n",
        "                                       dataset[\"text\"],\n",
        "                                       dataset[\"label\"])\n",
        "mnist_dataloader = DataLoader(mnist_dataset, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "ZI0JGHgD0gtI"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalMnistClassifier(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, n_hidden, n_layers, n_out):\n",
        "    super(MultimodalMnistClassifier, self).__init__()\n",
        "    # LSTM\n",
        "    self.vocab_size, self.n_hidden, self.n_out, self.n_layers = vocab_size, n_hidden, n_out, n_layers\n",
        "    self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.emb.weight.requires_grad = True\n",
        "    self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, dropout=0.2, batch_first=True)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.lstm_fc = nn.Linear(self.n_hidden, 128)\n",
        "\n",
        "    # CNN\n",
        "    self.conv1 = nn.Sequential(         \n",
        "            nn.Conv2d(\n",
        "                in_channels=1,              \n",
        "                out_channels=16,            \n",
        "                kernel_size=5,              \n",
        "                stride=1,                   \n",
        "                padding=2,                  \n",
        "            ),                              \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(kernel_size=2),    \n",
        "        )\n",
        "    self.conv2 = nn.Sequential(         \n",
        "        nn.Conv2d(16, 32, 5, 1, 2),     \n",
        "        nn.ReLU(),                      \n",
        "        nn.MaxPool2d(2),                \n",
        "    )\n",
        "\n",
        "    self.conv3 = nn.Sequential(         \n",
        "        nn.Conv2d(32, 64, 5, 1, 2),     \n",
        "        nn.ReLU(),                      \n",
        "        nn.MaxPool2d(2),                \n",
        "    )\n",
        "\n",
        "    self.conv4 = nn.Sequential(         \n",
        "        nn.Conv2d(64, 32, 3, 1, 2),     \n",
        "        nn.ReLU(),                      \n",
        "        nn.MaxPool2d(2),                \n",
        "    )\n",
        "    # Concat\n",
        "    self.combined_fc = nn.Linear(640, 128)\n",
        "    self.output_fc = nn.Linear(128, n_out)\n",
        "\n",
        "  def lstm_encoder(self, lstm_inp):\n",
        "    batch_size = lstm_inp.size(0)\n",
        "    hidden = self.init_hidden(batch_size)\n",
        "    lstm_inp = lstm_inp.long()\n",
        "    embeds = self.emb(lstm_inp)\n",
        "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    lstm_out = self.dropout(lstm_out[:, -1])\n",
        "    lstm_out = F.relu(self.lstm_fc(lstm_out))\n",
        "    return lstm_out\n",
        "  \n",
        "  def cnn_encoder(self, cnn_inp):\n",
        "    x = self.conv1(cnn_inp)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv4(x)\n",
        "    cnn_out = x.view(x.size(0), -1)\n",
        "    return cnn_out\n",
        "\n",
        "  def forward(self, cnn_inp, lstm_inp):\n",
        "    cnn_out = self.cnn_encoder(cnn_inp)\n",
        "    lstm_out = self.lstm_encoder(lstm_inp)\n",
        "    combined_inp = torch.cat((cnn_out, lstm_out), 1)\n",
        "    x_comb = F.relu(self.combined_fc(combined_inp))\n",
        "    x = self.output_fc(x_comb)\n",
        "    # x = torch.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(DEVICE),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(DEVICE))\n",
        "    return hidden"
      ],
      "metadata": {
        "id": "Shgd7aA6iJgi"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultimodalMnistClassifier(10, 100, HIDDEN_DIM, N_LAYERS, 1)\n",
        "model.to(DEVICE)\n",
        "criterion = nn.BCEWithLogitsLoss() # this means the sigmoid is INCORPORATED into the loss!!\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "G3KDI1NKk9E1"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for i in range(EPOCHS):\n",
        "  total_acc_train = 0\n",
        "  total_loss_train = 0\n",
        "\n",
        "  for cnn, lstm, label in mnist_dataloader:\n",
        "    lstm_inp, cnn_inp, label = lstm.to(DEVICE), cnn.to(DEVICE), label.to(DEVICE)\n",
        "    model.zero_grad()\n",
        "    output = model(cnn_inp, lstm_inp)\n",
        "    loss = criterion(torch.round(torch.sigmoid(output.squeeze())), label.float())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "      acc = torch.abs(torch.round(torch.sigmoid(output.squeeze())) - label.float()).view(-1)\n",
        "      acc = (1. - acc.sum() / acc.size()[0])\n",
        "      total_acc_train += acc.item()\n",
        "      total_loss_train += loss.item()\n",
        "  print(\"Accuracy\", total_acc_train / len(mnist_dataloader))\n",
        "  print(\"Loss\", total_loss_train / len(mnist_dataloader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvmjyNiBlSey",
        "outputId": "425cf8c3-46b6-4f1b-bce2-2321bc6b8da6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.5\n",
            "Loss 0.6931471824645996\n",
            "Accuracy 0.5\n",
            "Loss 0.6931471824645996\n",
            "Accuracy 0.5\n",
            "Loss 0.6931471824645996\n",
            "Accuracy 0.5\n",
            "Loss 0.6931471824645996\n",
            "Accuracy 0.5\n",
            "Loss 0.6931471824645996\n",
            "Accuracy 0.5\n",
            "Loss 0.6931471824645996\n",
            "Accuracy 0.5\n",
            "Loss 0.6931471824645996\n",
            "Accuracy 0.5\n",
            "Loss 0.6931471824645996\n",
            "Accuracy 0.5\n",
            "Loss 0.6931471824645996\n",
            "Accuracy 0.5\n",
            "Loss 0.6931471824645996\n",
            "Accuracy 0.5\n",
            "Loss 0.6931471824645996\n",
            "Accuracy 0.5\n",
            "Loss 0.6931471824645996\n",
            "Accuracy 0.5\n",
            "Loss 0.6931471824645996\n",
            "Accuracy 0.5\n",
            "Loss 0.6931471824645996\n",
            "Accuracy 0.5\n",
            "Loss 0.6931471824645996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cKWPARDrmDRh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}