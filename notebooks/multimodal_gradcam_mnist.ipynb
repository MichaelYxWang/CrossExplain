{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multimodal-gradcam-mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6TYShLC5ydW",
        "outputId": "2189e5d2-e0e3-42bf-a2aa-1daf67bedec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 28.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 60.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 47.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Create multmiodal dataloader for this\n",
        "2. Create a CNN-LSTM that can train and inference\n",
        "3. Add gradcam and all other stuff"
      ],
      "metadata": {
        "id": "u9sN9-Z4s8w-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import vgg19, resnet18, densenet161\n",
        "\n",
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from torch.optim import AdamW, SGD, Adam\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import pickle"
      ],
      "metadata": {
        "id": "6sV60oWZ6DRC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Path Vairables\n",
        "ROOT_DIR =  \"drive/MyDrive/11877-AMMML/dataset/\"\n",
        "DATASET_DIR = ROOT_DIR"
      ],
      "metadata": {
        "id": "ynhiZiXF6MSf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "LEARNING_RATE = 2e-4 # 0.001\n",
        "HIDDEN_DIM = 64 # 64\n",
        "N_LAYERS = 8 # 2\n",
        "EPOCHS = 20\n",
        "CLIP = 5\n",
        "DROPOUT = 0.1\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "dy_UjPCWlvZs"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pickle.load(open(DATASET_DIR + 'collaged_MNIST_maxpostext.p','rb'))"
      ],
      "metadata": {
        "id": "vnUa92qa6Wy-"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalMnistDataset(Dataset):\n",
        "  \"\"\"Dataset containing pairs of MNIST collaged images and text descriptions.\"\"\"\n",
        "  def __init__(self, images, texts, labels):\n",
        "    super(MultimodalMnistDataset, self).__init__()\n",
        "    self.images = [np.expand_dims(img.astype(np.float32) / 255, 0) for img in images]\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.images[idx], self.texts[idx], self.labels[idx]\n"
      ],
      "metadata": {
        "id": "19TkLQizv0Hd"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# texts = [\" \".join(list(map(str, x.tolist()))) for x in dataset[\"text\"]]\n",
        "texts = [x for x in dataset[\"text\"]]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "tokenized_texts = tokenizer.texts_to_sequences(texts)\n",
        "tokenized_texts = pad_sequences(tokenized_texts, maxlen=4)\n",
        "mnist_dataset = MultimodalMnistDataset(dataset[\"image\"],\n",
        "                                      #  dataset[\"text\"],\n",
        "                                       tokenized_texts,\n",
        "                                       dataset[\"label\"])\n",
        "mnist_dataloader = DataLoader(mnist_dataset, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "ZI0JGHgD0gtI"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalMnistClassifier(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, n_hidden, n_layers, n_out):\n",
        "    super(MultimodalMnistClassifier, self).__init__()\n",
        "    # LSTM\n",
        "    self.vocab_size, self.n_hidden, self.n_out, self.n_layers = vocab_size, n_hidden, n_out, n_layers\n",
        "    self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.emb.weight.requires_grad = True\n",
        "    self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, dropout=0.2, batch_first=True)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.lstm_fc = nn.Linear(self.n_hidden, 128)\n",
        "\n",
        "    # CNN\n",
        "    self.conv1 = nn.Sequential(         \n",
        "            nn.Conv2d(\n",
        "                in_channels=1,              \n",
        "                out_channels=16,            \n",
        "                kernel_size=5,              \n",
        "                stride=1,                   \n",
        "                padding=2,                  \n",
        "            ),                              \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(kernel_size=2),    \n",
        "        )\n",
        "    self.conv2 = nn.Sequential(         \n",
        "        nn.Conv2d(16, 32, 5, 1, 2),     \n",
        "        nn.ReLU(),                      \n",
        "        nn.MaxPool2d(2),                \n",
        "    )\n",
        "\n",
        "    self.conv3 = nn.Sequential(         \n",
        "        nn.Conv2d(32, 64, 5, 1, 2),     \n",
        "        nn.ReLU(),                      \n",
        "        nn.MaxPool2d(2),                \n",
        "    )\n",
        "\n",
        "    self.conv4 = nn.Sequential(         \n",
        "        nn.Conv2d(64, 32, 3, 1, 2),     \n",
        "        nn.ReLU(),                      \n",
        "        nn.MaxPool2d(2),                \n",
        "    )\n",
        "    # Concat\n",
        "    self.combined_fc = nn.Linear(640, 128)\n",
        "    self.output_fc = nn.Linear(128, n_out)\n",
        "\n",
        "  def lstm_encoder(self, lstm_inp):\n",
        "    batch_size = lstm_inp.size(0)\n",
        "    hidden = self.init_hidden(batch_size)\n",
        "    lstm_inp = lstm_inp.long()\n",
        "    embeds = self.emb(lstm_inp)\n",
        "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    lstm_out = self.dropout(lstm_out[:, -1])\n",
        "    lstm_out = F.relu(self.lstm_fc(lstm_out))\n",
        "    return lstm_out\n",
        "  \n",
        "  def cnn_encoder(self, cnn_inp):\n",
        "    x = self.conv1(cnn_inp)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv4(x)\n",
        "    \n",
        "    cnn_out = x.view(x.size(0), -1)\n",
        "    return cnn_out\n",
        "\n",
        "  def forward(self, cnn_inp, lstm_inp):\n",
        "    cnn_out = self.cnn_encoder(cnn_inp)\n",
        "    lstm_out = self.lstm_encoder(lstm_inp)\n",
        "    combined_inp = torch.cat((cnn_out, lstm_out), 1)\n",
        "    x_comb = F.relu(self.combined_fc(combined_inp))\n",
        "    x = self.output_fc(x_comb)\n",
        "    return x\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(DEVICE),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(DEVICE))\n",
        "    return hidden"
      ],
      "metadata": {
        "id": "Shgd7aA6iJgi"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultimodalMnistClassifier(10, 100, HIDDEN_DIM, N_LAYERS, 1)\n",
        "model.to(DEVICE)\n",
        "criterion = nn.BCEWithLogitsLoss() # this means the sigmoid is INCORPORATED into the loss!!\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "G3KDI1NKk9E1"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for i in range(EPOCHS):\n",
        "  total_acc_train = 0\n",
        "  total_loss_train = 0\n",
        "\n",
        "  for cnn, lstm, label in mnist_dataloader:\n",
        "    lstm_inp, cnn_inp, label = lstm.to(DEVICE), cnn.to(DEVICE), label.to(DEVICE)\n",
        "    model.zero_grad()\n",
        "    output = model(cnn_inp, lstm_inp)\n",
        "    loss = criterion(output.squeeze(), label.float())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "      acc = torch.abs(torch.round(torch.sigmoid(output.squeeze())) - label.float()).view(-1)\n",
        "      acc = (1. - acc.sum() / acc.size()[0])\n",
        "      total_acc_train += acc.item()\n",
        "      total_loss_train += loss.item()\n",
        "  print(\"Accuracy\", total_acc_train / len(mnist_dataloader))\n",
        "  print(\"Loss\", total_loss_train / len(mnist_dataloader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvmjyNiBlSey",
        "outputId": "6c8b8d8c-f057-41ac-b353-53a6318de555"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.4981715425531915\n",
            "Loss 0.6935650633370622\n",
            "Accuracy 0.49617686170212766\n",
            "Loss 0.6934875760306703\n",
            "Accuracy 0.49933510638297873\n",
            "Loss 0.693387379037573\n",
            "Accuracy 0.4953457446808511\n",
            "Loss 0.6933392324346177\n",
            "Accuracy 0.49833776595744683\n",
            "Loss 0.693295308567108\n",
            "Accuracy 0.4998337765957447\n",
            "Loss 0.6933254504457433\n",
            "Accuracy 0.4965093085106383\n",
            "Loss 0.6933272569737536\n",
            "Accuracy 0.5023271276595744\n",
            "Loss 0.6932055981869393\n",
            "Accuracy 0.515625\n",
            "Loss 0.6929035995234835\n",
            "Accuracy 0.5289228723404256\n",
            "Loss 0.6917530174585099\n",
            "Accuracy 0.5453789893617021\n",
            "Loss 0.6887271182334169\n",
            "Accuracy 0.5644946808510638\n",
            "Loss 0.681879274705623\n",
            "Accuracy 0.5939162234042553\n",
            "Loss 0.6703672748296818\n",
            "Accuracy 0.6186835106382979\n",
            "Loss 0.6537140119583049\n",
            "Accuracy 0.6504321808510638\n",
            "Loss 0.6305852107545162\n",
            "Accuracy 0.682845744680851\n",
            "Loss 0.603309451107015\n",
            "Accuracy 0.7137632978723404\n",
            "Loss 0.5674235430803705\n",
            "Accuracy 0.7448470744680851\n",
            "Loss 0.5291702148128063\n",
            "Accuracy 0.7744348404255319\n",
            "Loss 0.4883392757557808\n",
            "Accuracy 0.7977061170212766\n",
            "Loss 0.450586818713457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = 3\n",
        "image_bytes = dataset[\"image\"][index]\n",
        "\n",
        "plt.imshow(image_bytes, cmap=plt.cm.gray)\n",
        "print(\"Image label is\", dataset[\"label\"][index])\n",
        "print(\"Image text is\", dataset[\"text\"][index])\n",
        "\n",
        "\n",
        "cnn_inp = torch.from_numpy(np.expand_dims(np.expand_dims(image_bytes.astype(np.float32) / 255, 0), 0))\n",
        "cnn_inp = cnn_inp.to(DEVICE)\n",
        "lstm_inp = torch.from_numpy(np.expand_dims(tokenized_texts[index], 0))\n",
        "lstm_inp = lstm_inp.to(DEVICE)\n",
        "output = model(cnn_inp, lstm_inp)"
      ],
      "metadata": {
        "id": "cKWPARDrmDRh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "bd233c24-67f8-41d2-d8fa-fe53073c9a26"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image label is 1.0\n",
            "Image text is upper right\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NytOvGDjl5vb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}